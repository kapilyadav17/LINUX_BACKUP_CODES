{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b8c86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install together pandas matplotlib seaborn openpyxl python-docx PyMuPDF pillow pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f0add4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import together\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import docx\n",
    "import fitz \n",
    "from dotenv import load_dotenv\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f3e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7e49cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(file_path):\n",
    "    if file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_path.endswith('.xlsx'):\n",
    "        return pd.read_excel(file_path)\n",
    "    elif file_path.endswith('.txt'):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return f.read()\n",
    "    elif file_path.endswith('.docx'):\n",
    "        doc = docx.Document(file_path)\n",
    "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
    "    elif file_path.endswith('.pdf'):\n",
    "        doc = fitz.open(file_path)\n",
    "        return \"\\n\".join([page.get_text() for page in doc])\n",
    "    elif file_path.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        img = Image.open(file_path)\n",
    "        return pytesseract.image_to_string(img)\n",
    "    else:\n",
    "        return \"Unsupported file type\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d0c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llama(prompt: str):\n",
    "    response = together.Complete.create(\n",
    "        model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    print(\"API response:\", response)  \n",
    "    return response['output']['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7777661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_llama(prompt: str):\n",
    "    response = together.Complete.create(\n",
    "        model=\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=1024,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    if 'output' in response:\n",
    "        return response['output']['choices'][0]['text'].strip()\n",
    "    else:\n",
    "        print(\"API Error:\", response)\n",
    "        return \"API Error: \" + str(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef4e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_structured(df, question):\n",
    "    sample = df.head(5).to_string()\n",
    "    return f\"\"\"\n",
    "You are a data analyst. Here's a sample of the dataset:\n",
    "\n",
    "{sample}\n",
    "\n",
    "{question}\n",
    "Provide detailed analysis.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b26fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_unstructured(text, question):\n",
    "    return f\"\"\"\n",
    "You are an intelligent document analyst. Here's a portion of the document:\n",
    "\n",
    "{text[:1000]}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "26c9d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_summary(df):\n",
    "    print(\"Summary:\")\n",
    "    print(df.describe(include='all'))\n",
    "    print(\"\\nMissing Values:\\n\", df.isnull().sum())\n",
    "\n",
    "def plot_column_counts(df, column):\n",
    "    df[column].value_counts().plot(kind='bar', title=f'Distribution of {column}')\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "def plot_numeric_distribution(df, column):\n",
    "    sns.histplot(df[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbb8faf",
   "metadata": {},
   "source": [
    "UPLOADING FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c5500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Order ID Customer Name     Product         Category  Quantity  Price  \\\n",
      "0      1001    John Smith  Smartphone      Electronics         1    699   \n",
      "1      1002   Alice Brown      Laptop      Electronics         1    999   \n",
      "2      1003     Bob Davis     T-shirt          Apparel         3     25   \n",
      "3      1004    Eve Turner     Blender  Home Appliances         1     45   \n",
      "4      1005     Raj Patel       Shoes         Footwear         2     50   \n",
      "\n",
      "   Total  Order Date Country  \n",
      "0    699  2023-01-15     USA  \n",
      "1    999  2023-01-16      UK  \n",
      "2     75  2023-01-17   India  \n",
      "3     45  2023-01-18  Canada  \n",
      "4    100  2023-01-19   India  \n",
      "Summary:\n",
      "          Order ID Customer Name     Product     Category   Quantity  \\\n",
      "count     10.00000            10          10           10  10.000000   \n",
      "unique         NaN            10          10            4        NaN   \n",
      "top            NaN    John Smith  Smartphone  Electronics        NaN   \n",
      "freq           NaN             1           1            4        NaN   \n",
      "mean    1005.50000           NaN         NaN          NaN   1.400000   \n",
      "std        3.02765           NaN         NaN          NaN   0.699206   \n",
      "min     1001.00000           NaN         NaN          NaN   1.000000   \n",
      "25%     1003.25000           NaN         NaN          NaN   1.000000   \n",
      "50%     1005.50000           NaN         NaN          NaN   1.000000   \n",
      "75%     1007.75000           NaN         NaN          NaN   1.750000   \n",
      "max     1010.00000           NaN         NaN          NaN   3.000000   \n",
      "\n",
      "             Price       Total  Order Date Country  \n",
      "count    10.000000   10.000000          10      10  \n",
      "unique         NaN         NaN          10       4  \n",
      "top            NaN         NaN  2023-01-15     USA  \n",
      "freq           NaN         NaN           1       3  \n",
      "mean    267.800000  292.800000         NaN     NaN  \n",
      "std     337.306916  327.250091         NaN     NaN  \n",
      "min      25.000000   45.000000         NaN     NaN  \n",
      "25%      52.500000   76.250000         NaN     NaN  \n",
      "50%     100.000000  110.000000         NaN     NaN  \n",
      "75%     375.000000  412.500000         NaN     NaN  \n",
      "max     999.000000  999.000000         NaN     NaN  \n",
      "\n",
      "Missing Values:\n",
      " Order ID         0\n",
      "Customer Name    0\n",
      "Product          0\n",
      "Category         0\n",
      "Quantity         0\n",
      "Price            0\n",
      "Total            0\n",
      "Order Date       0\n",
      "Country          0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62197/3418193526.py:2: DeprecationWarning: Call to deprecated function create.\n",
      "  response = together.Complete.create(\n",
      "/home/kapil/Downloads/VSCODE/venv/lib/python3.12/site-packages/together/legacy/complete.py:23: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
      "  warnings.warn(API_KEY_WARNING)\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8. The maximum rate limit for this model is 0.6 queries and 180000000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m     final_prompt = prompt_unstructured(parsed_data, user_question)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Ask LLaMA\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m response = \u001b[43mask_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ§  Agent Response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mask_llama\u001b[39m\u001b[34m(prompt)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mask_llama\u001b[39m(prompt: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     response = \u001b[43mtogether\u001b[49m\u001b[43m.\u001b[49m\u001b[43mComplete\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m response:\n\u001b[32m      9\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response[\u001b[33m'\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/VSCODE/venv/lib/python3.12/site-packages/together/legacy/base.py:25\u001b[39m, in \u001b[36mdeprecated.<locals>.new_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_func\u001b[39m(*args, **kwargs):  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     20\u001b[39m     warnings.warn(\n\u001b[32m     21\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCall to deprecated function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m         category=\u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m     23\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m     24\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/VSCODE/venv/lib/python3.12/site-packages/together/legacy/complete.py:28\u001b[39m, in \u001b[36mComplete.create\u001b[39m\u001b[34m(cls, prompt, **kwargs)\u001b[39m\n\u001b[32m     24\u001b[39m     api_key = together.api_key\n\u001b[32m     26\u001b[39m client = together.Together(api_key=api_key)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m result = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, CompletionResponse)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/VSCODE/venv/lib/python3.12/site-packages/together/resources/completions.py:123\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, prompt, model, max_tokens, stop, temperature, top_p, top_k, repetition_penalty, presence_penalty, frequency_penalty, min_p, logit_bias, seed, stream, logprobs, echo, n, safety_model, **kwargs)\u001b[39m\n\u001b[32m     97\u001b[39m requestor = api_requestor.APIRequestor(\n\u001b[32m     98\u001b[39m     client=\u001b[38;5;28mself\u001b[39m._client,\n\u001b[32m     99\u001b[39m )\n\u001b[32m    101\u001b[39m parameter_payload = CompletionRequest(\n\u001b[32m    102\u001b[39m     model=model,\n\u001b[32m    103\u001b[39m     prompt=prompt,\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m     **kwargs,\n\u001b[32m    121\u001b[39m ).model_dump(exclude_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m response, _, _ = \u001b[43mrequestor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTogetherRequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompletions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameter_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, TogetherResponse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/VSCODE/venv/lib/python3.12/site-packages/together/abstract/api_requestor.py:249\u001b[39m, in \u001b[36mAPIRequestor.request\u001b[39m\u001b[34m(self, options, stream, remaining_retries, request_timeout)\u001b[39m\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m    232\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    233\u001b[39m     options: TogetherRequest,\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    241\u001b[39m ]:\n\u001b[32m    242\u001b[39m     result = \u001b[38;5;28mself\u001b[39m.request_raw(\n\u001b[32m    243\u001b[39m         options=options,\n\u001b[32m    244\u001b[39m         remaining_retries=remaining_retries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.retries,\n\u001b[32m    245\u001b[39m         stream=stream,\n\u001b[32m    246\u001b[39m         request_timeout=request_timeout,\n\u001b[32m    247\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     resp, got_stream = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m.api_key\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/VSCODE/venv/lib/python3.12/site-packages/together/abstract/api_requestor.py:635\u001b[39m, in \u001b[36mAPIRequestor._interpret_response\u001b[39m\u001b[34m(self, result, stream)\u001b[39m\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    633\u001b[39m     content = result.content.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m635\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    638\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    640\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    641\u001b[39m     \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    642\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/VSCODE/venv/lib/python3.12/site-packages/together/abstract/api_requestor.py:707\u001b[39m, in \u001b[36mAPIRequestor._interpret_response_line\u001b[39m\u001b[34m(self, rbody, rcode, rheaders, stream)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Handle streaming errors\u001b[39;00m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m200\u001b[39m <= rcode < \u001b[32m300\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handle_error_response(resp, rcode, stream_error=stream)\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {\"message\": \"You have reached the rate limit specific to this model meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8. The maximum rate limit for this model is 0.6 queries and 180000000 tokens per minute. This limit differs from the general rate limits published at Together AI rate limits documentation (https://docs.together.ai/docs/rate-limits). For inquiries about increasing your model-specific rate limit, please contact our sales team (https://www.together.ai/forms/contact-sales)\", \"type_\": \"model_rate_limit\"}"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"student_data.csv\" \n",
    "\n",
    "parsed_data = parse_file(file_path)\n",
    "\n",
    "if isinstance(parsed_data, pd.DataFrame):\n",
    "    print(parsed_data.head())\n",
    "    basic_summary(parsed_data)\n",
    "    user_question = input(\"Question : \")\n",
    "    final_prompt = prompt_structured(parsed_data, user_question)\n",
    "else:\n",
    "    print(parsed_data[:1000])\n",
    "    user_question = input(\"Question : \")\n",
    "    final_prompt = prompt_unstructured(parsed_data, user_question)\n",
    "\n",
    "\n",
    "response = ask_llama(final_prompt)\n",
    "print(\"\\nAgent Response:\\n\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255552e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
